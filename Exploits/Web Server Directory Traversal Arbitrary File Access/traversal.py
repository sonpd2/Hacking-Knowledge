import urllib2
import threading
import sys
import os

history = []

def Crawl(dict,target,folder):
	global history
	for uri in dict.readlines():
		uri = uri.replace("\n","").replace(" ","").replace("\r","").replace("//","\\")
		if len(uri) > 1:
			if uri not in history:
				history.append(uri)
				url = target + uri
				try:
					res = urllib2.urlopen(url,timeout = 10)
					htmlSource = res.read()
					print("Crawl PATH {}".format(url))
					file = open(folder + "/"  + uri.split("/")[-1],'w')
					file.write("-----------" + url + "-----------\n")
					file.write(htmlSource)
					file.close()
					res.close()
				except Exception as e:
					print(e)

if __name__ == "__main__":
	
	if not os.path.exists(sys.argv[1]):
		os.mkdir(sys.argv[1])
		
	dict = file(sys.argv[2],'r')
	list_thread = []
	while True:
		if len(list_thread) < int(sys.argv[3]):
			thread = threading.Thread(target=Crawl, args=(dict,sys.argv[4],sys.argv[1],)) 
			list_thread.append(thread)
		else:
			break

	for i in list_thread:
		i.start()
		
	for i in list_thread:
		i.join()

		

